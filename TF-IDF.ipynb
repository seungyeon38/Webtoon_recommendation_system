{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYIoNMz6bsGim6WnEvfMxD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seungyeon38/Webtoon_recommendation_system/blob/master/TF-IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1AEKymwhWxV",
        "outputId": "2690e291-8eb4-4c9f-8a1c-61a3c489eae6"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "string.punctuation\n",
        "from math import log\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from gensim.summarization import keywords\n",
        "\n",
        "\n",
        "!pip install gensim\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tBTfyqehdID"
      },
      "source": [
        "comic = pd.read_csv('./sample_data/Webtoon Dataset.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsmb3sHGhdev"
      },
      "source": [
        "# 축약형 \n",
        "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"she's\": \"she is\"}\n",
        "\n",
        "def regularize_contractions(text):\n",
        "\n",
        "    contractionfree_list = []\n",
        "    \n",
        "    for word in text.split(\" \"):\n",
        "      if word in contractions.keys():\n",
        "        contractionfree_list.append(contractions[word])\n",
        "      else :\n",
        "        contractionfree_list.append(word)\n",
        "      \n",
        "    return \" \".join(contractionfree_list)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPpeOq8H5WV_"
      },
      "source": [
        "#defining the function to remove punctuation\n",
        "# 글자가 puctuation에 해당하면 \" \"를, 해당하지 않으면 글자 그대로\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree_list = []\n",
        "\n",
        "    for word in text:\n",
        "      if word in string.punctuation:\n",
        "        punctuationfree_list.append(\" \")\n",
        "      else :\n",
        "        punctuationfree_list.append(word)\n",
        "    return \"\".join(punctuationfree_list)\n",
        "\n",
        "\n",
        "# def remove_punctuation(text):\n",
        "#     punctuationfree = \"\".join([i for i in text if i not in string.punctuation])\n",
        "#     return punctuationfree"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_YijrK_flsD"
      },
      "source": [
        "# #defining function for tokenization\n",
        "# 숫자 포함 토큰화\n",
        "# def tokenization(text):\n",
        "#     tokens = re.split('\\W+', text)\n",
        "\n",
        "#     return tokens\n",
        "\n",
        "# 영어만 단어단위로 토큰화\n",
        "p = re.compile('[a-z]+')\n",
        "\n",
        "def tokenization(text):\n",
        "    # print(text)\n",
        "    result = p.findall(text)\n",
        "    # print(result)\n",
        "    return result"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t9lmVILpJ5J"
      },
      "source": [
        "#stop words present in the library\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "# print(stopwords[:20]) \n",
        "\n",
        "#defining the function to remove stopwords from tokenized text\n",
        "# stopwords 제거 \n",
        "def remove_stopwords(text):\n",
        "    for word in text:\n",
        "      output= [word for word in text if word not in stopwords]\n",
        "    return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxsyAbTHb5Wl",
        "outputId": "f6b192ae-e07f-427d-cf5d-bbe9c9bf9e93"
      },
      "source": [
        "#setting lower case\n",
        "comic['Lower_Summary'] = comic['Summary'].apply(lambda x: x.lower())\n",
        "comic['Lower_Summary'].head(3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    she's young, single and about to achieve her d...\n",
              "1    after binge-watching beauty videos online, a s...\n",
              "2    after making a grisly discovery in the country...\n",
              "Name: Lower_Summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ksxt3pIkA7O",
        "outputId": "a267dcef-31ec-4cfe-f640-7e3a3429fb7c"
      },
      "source": [
        "comic['No_Contraction_Summary'] = comic['Lower_Summary'].apply(lambda x:regularize_contractions(x))\n",
        "comic['No_Contraction_Summary'].head(3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    she is young, single and about to achieve her ...\n",
              "1    after binge-watching beauty videos online, a s...\n",
              "2    after making a grisly discovery in the country...\n",
              "Name: No_Contraction_Summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6dBD6jokDNl",
        "outputId": "2739344a-354d-41c2-91ea-8c433e42992c"
      },
      "source": [
        "comic['Clean_Summary'] = comic['No_Contraction_Summary'].apply(lambda x:remove_punctuation(x))\n",
        "comic['Clean_Summary'].head(3)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    she is young  single and about to achieve her ...\n",
              "1    after binge watching beauty videos online  a s...\n",
              "2    after making a grisly discovery in the country...\n",
              "Name: Clean_Summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu8IBMAomStz",
        "outputId": "ec3e5450-2066-443f-daa7-96784a15751f"
      },
      "source": [
        "#applying function to the column\n",
        "comic['Tokenied_Summary'] = comic['Clean_Summary'].apply(lambda x: tokenization(x))\n",
        "\n",
        "comic['Tokenied_Summary'].head(3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [she, is, young, single, and, about, to, achie...\n",
              "1    [after, binge, watching, beauty, videos, onlin...\n",
              "2    [after, making, a, grisly, discovery, in, the,...\n",
              "Name: Tokenied_Summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CpN1ehSdabd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db947181-9078-4a50-f93c-18128f1caf13"
      },
      "source": [
        "#applying the function\n",
        "comic['No_Stopwords_Summary'] = comic['Tokenied_Summary'].apply(lambda x:remove_stopwords(x))\n",
        "\n",
        "comic['No_Stopwords_Summary'].head(3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [young, single, achieve, dream, creating, incr...\n",
              "1    [binge, watching, beauty, videos, online, shy,...\n",
              "2    [making, grisly, discovery, countryside, small...\n",
              "Name: No_Stopwords_Summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_vTgWNOcAeo",
        "outputId": "a325c2c5-d797-46f0-8e97-88d3a629cd56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def extract_noun(text):\n",
        "  part_of_speech_text = pos_tag(text)\n",
        "  result = []\n",
        "  for word in part_of_speech_text:\n",
        "    if word[1] == \"NN\" or word[1] == \"NNS\":\n",
        "        result.append(word[0])\n",
        "    \n",
        "  return result; \n",
        "\n",
        "comic['Noun_Summary'] = comic['No_Stopwords_Summary'].apply(lambda x:extract_noun(x))\n",
        "\n",
        "comic['Noun_Summary'].head(3)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [dream, videogames, life, punch, streamer, gam...\n",
              "1    [binge, beauty, online, book, fan, masters, ma...\n",
              "2    [town, book, editor, life, mafia, lord, step, ...\n",
              "Name: Noun_Summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqFR0C4gddRw"
      },
      "source": [
        "# df1 = comic['No_Stopwords_Summary']\n",
        "# doc_words_list = df1.values.tolist()\n",
        "\n",
        "# print(doc_words_list)\n",
        "\n",
        "\n",
        "def wordlist_to_string(words_list) :\n",
        "  doc = []\n",
        "  for doc_words in words_list :\n",
        "    doc.append(\" \".join(doc))\n",
        "  return doc\n",
        "\n",
        "\n",
        "comic['Summary_final'] = comic['Noun_Summary'].apply(lambda x:wordlist_to_string(x))\n",
        "\n",
        "\n",
        "comic['Summary_final'].head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW6gJoIYxfhl"
      },
      "source": [
        "def extract_keywords(doc):\n",
        "  keyword_list = []\n",
        "  keyword_string = keywords(doc)\n",
        "  keyword_list = keyword_string.split('\\n')\n",
        "\n",
        "  return keyword_list;\n",
        "\n",
        "\n",
        "comic['Summary_Keywords'] = comic['Summary_final'].apply(lambda x:extract_keywords(x))\n",
        "\n",
        "\n",
        "comic.head(3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxGei5956BLs"
      },
      "source": [
        "vocab_list = list(set(word for doc in doc_words_list for word in doc))\n",
        "vocab_list.sort()\n",
        "\n",
        "docs_num = len(docs) # 총 문서의 수\n",
        "\n",
        "def tf(t, d):\n",
        "    return d.count(t)\n",
        "\n",
        "def idf(t):\n",
        "    df = 0\n",
        "    for doc in docs:\n",
        "        df += t in doc\n",
        "    return log(docs_num/(df + 1))\n",
        "\n",
        "def tfidf(t, d):\n",
        "    return tf(t,d)* idf(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PlzEMHF6DE7"
      },
      "source": [
        "result = []\n",
        "# total_result = [0 for i in range(len(vocab_list))] # 단어 개수 세려고 만든 리스트\n",
        "\n",
        "for i in range(docs_num): # 각 문서에 대해서 아래 명령을 수행\n",
        "    result.append([])\n",
        "    doc = docs[i]\n",
        "    for j in range(len(vocab_list)):\n",
        "        vocab = vocab_list[j]        \n",
        "        result[-1].append(tf(vocab, doc)) # 각 문서에서 각 단어의 수를 세어서 result list에 넣음 \n",
        "        total_result[j] += tf(vocab, doc)\n",
        "\n",
        "print(total_result)\n",
        "\n",
        "# total_result_1_num = 0\n",
        "# total_result_big_num = 0\n",
        "\n",
        "# for i in total_result:\n",
        "#   if i==1:\n",
        "#     total_result_1_num += 1 \n",
        "#   elif i>20:\n",
        "#     total_result_big_num += 1\n",
        "    \n",
        "# print('total_result_1_num')\n",
        "# print(total_result_1_num) # 2530 -> 2997\n",
        "\n",
        "# print('total_result_big_num')\n",
        "# print(total_result_big_num) # 481 -> 113\n",
        "\n",
        "tf_ = pd.DataFrame(result, columns = vocab_list)\n",
        "tf_\n",
        "# tf_.shape #(569, 5696) -> (569, 5443)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSTFjVEunpzg"
      },
      "source": [
        "result = []\n",
        "for j in range(len(vocab_list)):\n",
        "    vocab = vocab_list[j]\n",
        "    result.append(idf(vocab))\n",
        "\n",
        "idf_ = pd.DataFrame(result, index = vocab_list, columns = [\"IDF\"])\n",
        "idf_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9EloXgvqDrw"
      },
      "source": [
        "result = []\n",
        "for i in tqdm(range(docs_num)):\n",
        "    result.append([])\n",
        "    d = docs[i]\n",
        "    for j in range(len(vocab_list)):\n",
        "        vocab = vocab_list[j]\n",
        "        result[-1].append(tfidf(vocab,d))\n",
        "\n",
        "tfidf_ = pd.DataFrame(result, columns = vocab_list)\n",
        "tfidf_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLRxdHLNv_Ma"
      },
      "source": [
        "print(result[500])\n",
        "# print(len(result))\n",
        "\n",
        "result_dictionary_list = []\n",
        "\n",
        "# for i in range(len(result)):\n",
        "#    result_dictionary = []\n",
        "#    result_dictionary.append({k: v for v, k in enumerate(result[i])})\n",
        "#    result_dictionary_list.append(result_dictionary)\n",
        "\n",
        "# print(result_dictionary_list[0])\n",
        "\n",
        "# print(dict(zip(result[0],range(len(result[0])))))\n",
        "\n",
        "\n",
        "# for i in result[0]:\n",
        "# print(sorted(result[0], reverse = True))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}