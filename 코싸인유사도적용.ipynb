{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "코싸인유사도적용.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seungyeon38/Webtoon_recommendation_system/blob/master/%EC%BD%94%EC%8B%B8%EC%9D%B8%EC%9C%A0%EC%82%AC%EB%8F%84%EC%A0%81%EC%9A%A9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1AEKymwhWxV",
        "outputId": "bf62eb77-5c4d-4797-a7b7-2f3b565c95df"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "string.punctuation\n",
        "from math import log\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from gensim.summarization import keywords\n",
        "from textblob import TextBlob\n",
        "\n",
        "!pip install gensim\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tBTfyqehdID"
      },
      "source": [
        "comic = pd.read_csv('./sample_data/Webtoon Dataset.csv')"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_comic = input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjOwaO2zv0sW",
        "outputId": "657c7bcc-feb9-48a4-926d-95b943281a9c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's Play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# selected_comic = comic.loc[(comic.Name == input_comic)]\n",
        "\n",
        "# print(selected_comic['Name'])"
      ],
      "metadata": {
        "id": "7UunDt7dvtYs"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsmb3sHGhdev"
      },
      "source": [
        "# 축약형 \n",
        "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"she's\": \"she is\"}\n",
        "\n",
        "def regularize_contractions(text):\n",
        "\n",
        "    contractionfree_list = []\n",
        "    \n",
        "    for word in text.split(\" \"):\n",
        "      if word in contractions.keys():\n",
        "        contractionfree_list.append(contractions[word])\n",
        "      else :\n",
        "        contractionfree_list.append(word)\n",
        "      \n",
        "    return \" \".join(contractionfree_list)\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPpeOq8H5WV_"
      },
      "source": [
        "#defining the function to remove punctuation\n",
        "# 글자가 puctuation에 해당하면 \" \"를, 해당하지 않으면 글자 그대로\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree_list = []\n",
        "\n",
        "    for word in text:\n",
        "      if word in string.punctuation:\n",
        "        punctuationfree_list.append(\" \")\n",
        "      else :\n",
        "        punctuationfree_list.append(word)\n",
        "    return \"\".join(punctuationfree_list)\n",
        "\n",
        "\n",
        "# def remove_punctuation(text):\n",
        "#     punctuationfree = \"\".join([i for i in text if i not in string.punctuation])\n",
        "#     return punctuationfree"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_YijrK_flsD"
      },
      "source": [
        "# #defining function for tokenization\n",
        "# 숫자 포함 토큰화\n",
        "# def tokenization(text):\n",
        "#     tokens = re.split('\\W+', text)\n",
        "\n",
        "#     return tokens\n",
        "\n",
        "# 영어만 단어단위로 토큰화\n",
        "p = re.compile('[a-z]+')\n",
        "\n",
        "def tokenization(text):\n",
        "    # print(text)\n",
        "    result = p.findall(text)\n",
        "    # print(result)\n",
        "    return result"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t9lmVILpJ5J"
      },
      "source": [
        "#stop words present in the library\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "# print(stopwords[:20]) \n",
        "\n",
        "#defining the function to remove stopwords from tokenized text\n",
        "# stopwords 제거 \n",
        "def remove_stopwords(text):\n",
        "    for word in text:\n",
        "      output= [word for word in text if word not in stopwords]\n",
        "    return output"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxsyAbTHb5Wl",
        "outputId": "b294b59e-2d62-4160-ffd3-431afac2aa80"
      },
      "source": [
        "#setting lower case\n",
        "comic['Lower_Summary'] = comic['Summary'].apply(lambda x: x.lower())\n",
        "comic['Lower_Summary'].head(3)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    she's young, single and about to achieve her d...\n",
              "1    after binge-watching beauty videos online, a s...\n",
              "2    after making a grisly discovery in the country...\n",
              "Name: Lower_Summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ksxt3pIkA7O",
        "outputId": "e3db6a2e-9270-4669-b8e9-4e4bf1935203"
      },
      "source": [
        "comic['No_Contraction_Summary'] = comic['Lower_Summary'].apply(lambda x:regularize_contractions(x))\n",
        "comic['No_Contraction_Summary'].head(3)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    she is young, single and about to achieve her ...\n",
              "1    after binge-watching beauty videos online, a s...\n",
              "2    after making a grisly discovery in the country...\n",
              "Name: No_Contraction_Summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6dBD6jokDNl",
        "outputId": "c0c70603-b06d-4abe-9213-435dc102cb89"
      },
      "source": [
        "comic['Clean_Summary'] = comic['No_Contraction_Summary'].apply(lambda x:remove_punctuation(x))\n",
        "comic['Clean_Summary'].head(3)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    she is young  single and about to achieve her ...\n",
              "1    after binge watching beauty videos online  a s...\n",
              "2    after making a grisly discovery in the country...\n",
              "Name: Clean_Summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu8IBMAomStz",
        "outputId": "015f5512-21c3-454e-e22f-b8d38cb0312f"
      },
      "source": [
        "#applying function to the column\n",
        "comic['Tokenied_Summary'] = comic['Clean_Summary'].apply(lambda x: tokenization(x))\n",
        "\n",
        "comic['Tokenied_Summary'].head(3)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [she, is, young, single, and, about, to, achie...\n",
              "1    [after, binge, watching, beauty, videos, onlin...\n",
              "2    [after, making, a, grisly, discovery, in, the,...\n",
              "Name: Tokenied_Summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CpN1ehSdabd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e05b3e-a681-491b-b0f3-2fe62cc47d34"
      },
      "source": [
        "#applying the function\n",
        "comic['No_Stopwords_Summary'] = comic['Tokenied_Summary'].apply(lambda x:remove_stopwords(x))\n",
        "\n",
        "comic['No_Stopwords_Summary'].head(3)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [young, single, achieve, dream, creating, incr...\n",
              "1    [binge, watching, beauty, videos, online, shy,...\n",
              "2    [making, grisly, discovery, countryside, small...\n",
              "Name: No_Stopwords_Summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_no_verb(text):\n",
        "  part_of_speech_text = pos_tag(text)\n",
        "  result = []\n",
        "  for word in part_of_speech_text:\n",
        "    if word[1] != \"VB\" and word[1] != \"VBD\" and word[1] != \"VBG\" and word[1] != \"VBN\" and word[1] != \"VBP\" and word[1] != \"VBZ\":\n",
        "        result.append(word[0])\n",
        "  return result; \n",
        "\n",
        "comic['No_Verb_Summary'] = comic['No_Stopwords_Summary'].apply(lambda x:extract_no_verb(x))\n",
        "\n",
        "comic['No_Verb_Summary'].head(3)"
      ],
      "metadata": {
        "id": "4LON9acgf0xx",
        "outputId": "56c2c8d4-34df-492b-a1e1-397ca247e822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [young, single, achieve, dream, incredible, vi...\n",
              "1    [binge, beauty, videos, online, shy, comic, bo...\n",
              "2    [grisly, discovery, countryside, small, town, ...\n",
              "Name: No_Verb_Summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqFR0C4gddRw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03760a6-2fd9-4219-993e-d9846ef0473f"
      },
      "source": [
        "# df1 = comic['No_Stopwords_Summary']\n",
        "# doc_words_list = df1.values.tolist()\n",
        "\n",
        "# print(doc_words_list)\n",
        "\n",
        "\n",
        "def wordlist_to_string(words_list) :\n",
        "  doc = []\n",
        "  for doc_word in words_list :\n",
        "    doc.append(doc_word)\n",
        "  return \" \".join(doc)\n",
        "\n",
        "\n",
        "#comic['Summary_final'] = comic['Stemmed_Summary'].apply(lambda x:wordlist_to_string(x))\n",
        "comic['Summary_final'] = comic['No_Verb_Summary'].apply(lambda x:wordlist_to_string(x))\n",
        "\n",
        "comic['Summary_final'].head(3)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    young single achieve dream incredible videogam...\n",
              "1    binge beauty videos online shy comic book fan ...\n",
              "2    grisly discovery countryside small town book e...\n",
              "Name: Summary_final, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW6gJoIYxfhl"
      },
      "source": [
        "def extract_keywords(doc):\n",
        "  keyword_list = []\n",
        "  keyword_string = keywords(doc)\n",
        "  keyword_list = keyword_string.split('\\n')\n",
        "\n",
        "  return keyword_list;\n",
        "\n",
        "\n",
        "comic['Summary_Keywords'] = comic['Summary_final'].apply(lambda x:extract_keywords(x))"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_3(keyword_list):\n",
        "  keyword_list_3 = keyword_list[0:3]\n",
        "\n",
        "  return keyword_list_3\n",
        "\n",
        "comic['Summary_Keywords_3'] = comic['Summary_Keywords'].apply(lambda x:keyword_3(x))\n",
        "\n",
        "comic.head(3)"
      ],
      "metadata": {
        "id": "xg6fXrcZtCV3",
        "outputId": "ea14f90f-ebfc-4286-cad5-f3babfd54860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Name</th>\n",
              "      <th>Writer</th>\n",
              "      <th>Likes</th>\n",
              "      <th>Genre</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Subscribers</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Update</th>\n",
              "      <th>Reading Link</th>\n",
              "      <th>Lower_Summary</th>\n",
              "      <th>No_Contraction_Summary</th>\n",
              "      <th>Clean_Summary</th>\n",
              "      <th>Tokenied_Summary</th>\n",
              "      <th>No_Stopwords_Summary</th>\n",
              "      <th>No_Verb_Summary</th>\n",
              "      <th>Summary_final</th>\n",
              "      <th>Summary_Keywords</th>\n",
              "      <th>Summary_Keywords_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Let's Play</td>\n",
              "      <td>Leeanne M. Krecic (Mongie)</td>\n",
              "      <td>30.6M</td>\n",
              "      <td>Romance</td>\n",
              "      <td>9.62</td>\n",
              "      <td>4.2M</td>\n",
              "      <td>She's young, single and about to achieve her d...</td>\n",
              "      <td>UP EVERY TUESDAY</td>\n",
              "      <td>https://www.webtoons.com/en/romance/letsplay/l...</td>\n",
              "      <td>she's young, single and about to achieve her d...</td>\n",
              "      <td>she is young, single and about to achieve her ...</td>\n",
              "      <td>she is young  single and about to achieve her ...</td>\n",
              "      <td>[she, is, young, single, and, about, to, achie...</td>\n",
              "      <td>[young, single, achieve, dream, creating, incr...</td>\n",
              "      <td>[young, single, achieve, dream, incredible, vi...</td>\n",
              "      <td>young single achieve dream incredible videogam...</td>\n",
              "      <td>[popular, videogames, single, stay, troublesome]</td>\n",
              "      <td>[popular, videogames, single]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>True Beauty</td>\n",
              "      <td>Yaongyi</td>\n",
              "      <td>39.9M</td>\n",
              "      <td>Romance</td>\n",
              "      <td>9.60</td>\n",
              "      <td>6.4M</td>\n",
              "      <td>After binge-watching beauty videos online, a s...</td>\n",
              "      <td>UP EVERY WEDNESDAY</td>\n",
              "      <td>https://www.webtoons.com/en/romance/truebeauty...</td>\n",
              "      <td>after binge-watching beauty videos online, a s...</td>\n",
              "      <td>after binge-watching beauty videos online, a s...</td>\n",
              "      <td>after binge watching beauty videos online  a s...</td>\n",
              "      <td>[after, binge, watching, beauty, videos, onlin...</td>\n",
              "      <td>[binge, watching, beauty, videos, online, shy,...</td>\n",
              "      <td>[binge, beauty, videos, online, shy, comic, bo...</td>\n",
              "      <td>binge beauty videos online shy comic book fan ...</td>\n",
              "      <td>[secret, beauty videos, status short]</td>\n",
              "      <td>[secret, beauty videos, status short]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Midnight Poppy Land</td>\n",
              "      <td>Lilydusk</td>\n",
              "      <td>10.4M</td>\n",
              "      <td>Romance</td>\n",
              "      <td>9.81</td>\n",
              "      <td>2.1M</td>\n",
              "      <td>After making a grisly discovery in the country...</td>\n",
              "      <td>UP EVERY SATURDAY</td>\n",
              "      <td>https://www.webtoons.com/en/romance/midnight-p...</td>\n",
              "      <td>after making a grisly discovery in the country...</td>\n",
              "      <td>after making a grisly discovery in the country...</td>\n",
              "      <td>after making a grisly discovery in the country...</td>\n",
              "      <td>[after, making, a, grisly, discovery, in, the,...</td>\n",
              "      <td>[making, grisly, discovery, countryside, small...</td>\n",
              "      <td>[grisly, discovery, countryside, small, town, ...</td>\n",
              "      <td>grisly discovery countryside small town book e...</td>\n",
              "      <td>[deeper dangerous underworld]</td>\n",
              "      <td>[deeper dangerous underworld]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                     Summary_Keywords_3\n",
              "0   0  ...          [popular, videogames, single]\n",
              "1   1  ...  [secret, beauty videos, status short]\n",
              "2   2  ...          [deeper dangerous underworld]\n",
              "\n",
              "[3 rows x 19 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#synsets(검색) -> 리스트\n",
        "\n",
        "#synset(신셋색인)  .definition() .examples()  .lemmas()  .hyponyms()\n",
        "# wordnet.synset('young.a.01').definition()\n",
        "# wordnet.synset('beauty.n.01').hypernym_paths()\n",
        "# word_beautiful = wordnet.synset('beautiful.a.01')\n",
        "# word_pretty = wordnet.synset('pretty.a.01')\n",
        "# print(word_pretty.wup_similarity(word_beautiful))\n",
        "\n",
        "# dog = wordnet.synset('dog.n.01')\n",
        "# cat = wordnet.synset('cat.n.01')\n",
        "\n",
        "# print(dog.wup_similarity(cat))\n",
        "\n",
        "\n",
        "!pip install textblob\n",
        "\n",
        "\n",
        "# pos_tag([\"beauty\"])\n",
        "\n",
        "# wordnet.synset('car.n.01').definition() wordnet.synset('car.n.01').examples() wordnet.synset('car.n.01').lemmas()\n",
        "\n",
        "# comic['Summary_Keywords_3'] = comic['Summary_Keywords'].apply(lambda x:keyword_3(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nuxt5l5o_BB",
        "outputId": "42ed4a38-b254-458c-f910-970bb16103bf"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "# word = Word(\"beauty\")\n",
        "# print(word.synsets)\n",
        "\n",
        "def keyword_synsets(keywords):\n",
        "  keyword_synsets = []\n",
        "  for i in range(len(keywords)):\n",
        "    keyword_synsets.append(keywords[i])\n",
        "    word = Word(keywords[i])\n",
        "    for synset in word.synsets:\n",
        "      for lemma in synset.lemmas():\n",
        "        if lemma.name() not in keyword_synsets:\n",
        "          keyword_synsets.append(lemma.name())\n",
        "\n",
        "  return keyword_synsets\n",
        "\n",
        "\n",
        "comic['Summary_Keywords_Synsets'] = comic['Summary_Keywords'].apply(lambda x:keyword_synsets(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_H6EGbjORSNJ",
        "outputId": "7998fb96-9d55-4e7c-ff25-24cc57afd1f4"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comic['Summary_Keywords'].head(1)\n",
        "comic['Summary_Keywords_Synsets'].head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Yf4ESd4Zck6",
        "outputId": "0250b957-0051-44d9-afeb-ff706567eb54"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [popular, democratic, pop, videogames, single,...\n",
              "Name: Summary_Keywords_Synsets, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "jYpGYoX32BpS",
        "outputId": "8fa55b84-f0bd-46cd-dcd8-c7c85e4cc44e"
      },
      "source": [
        "comic_df = comic[['id','Name', 'Writer', 'Genre', 'Rating', 'Summary_Keywords',\n",
        "                 'Summary_Keywords_3', 'Summary_Keywords_Synsets']]\n",
        "comic_df.head(1)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Name</th>\n",
              "      <th>Writer</th>\n",
              "      <th>Genre</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Summary_Keywords</th>\n",
              "      <th>Summary_Keywords_3</th>\n",
              "      <th>Summary_Keywords_Synsets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Let's Play</td>\n",
              "      <td>Leeanne M. Krecic (Mongie)</td>\n",
              "      <td>Romance</td>\n",
              "      <td>9.62</td>\n",
              "      <td>[popular, videogames, single, stay, troublesome]</td>\n",
              "      <td>[popular, videogames, single]</td>\n",
              "      <td>[popular, democratic, pop, videogames, single,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                           Summary_Keywords_Synsets\n",
              "0   0  ...  [popular, democratic, pop, videogames, single,...\n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv-v_XtT2VRd",
        "outputId": "36c612eb-92b6-4c1a-fdc5-fa056d05ed75"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "comic_df['Summary_literal'] = comic_df['Summary_Keywords_Synsets'].apply(lambda x : (' ').join(x))\n",
        "count_vect = CountVectorizer(min_df=0, ngram_range=(1,2))\n",
        "Summary_mat = count_vect.fit_transform(comic_df['Summary_literal'])\n",
        "print(Summary_mat.shape)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 14392)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 569개의 레코드, 2356개의 개별 단어 피처"
      ],
      "metadata": {
        "id": "kxtpFH6GLQ3z"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouiNDbbY4Mqu",
        "outputId": "d977d41a-ed27-4d01-da13-7d85c860efc1"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "Summary_sim = cosine_similarity(Summary_mat, Summary_mat)\n",
        "print(Summary_sim.shape)\n",
        "print(Summary_sim[:2])"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(569, 569)\n",
            "[[1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACEIjCcf4Zxq",
        "outputId": "7cd722fa-d7f6-4857-c05a-f7ec0fb4961f"
      },
      "source": [
        "Summary_sim_sorted_ind = Summary_sim.argsort()[:, ::-1]\n",
        "print(Summary_sim_sorted_ind[:1])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0  55 129   4 301 169 248 195 402 115 201  47 269 331 272 213 389 338\n",
            "  124 351 335 406 340 132  93 118  82 314 240 256  63 521 492 419  52 131\n",
            "  182 320 394 415 232 497 318 253  28 424 153 179 185 183 181 184 180 156\n",
            "  186 187 188 189 190 191 192 193 194 266 196 152 178 154 177 167 271 158\n",
            "  159 160 161 162 163 164 165 270 166 168 176 155 198 268 170 171 172 173\n",
            "  157 174 175 267 197 259 199 227 229 230 231 262 233 234 235 236 237 238\n",
            "  239 241 242 243 244 245 246 247 261 260 249 250 251 252 254 255 257 228\n",
            "  263 200 226 265 202 203 204 205 258 206 207 208 209 210 211 212 150 214\n",
            "  215 216 217 218 219 220 221 264 222 223 224 225 151 142 149  44  51  50\n",
            "   49  48  46  45  43 148  42  41  40  39  38  37  53  54  56  57  58  59\n",
            "   60  61  62  64  65  66  67  68  69  70  71  36  35  34  16   1   2   3\n",
            "    5   6   7   8   9  10  11  12  13  14  15  17  33  18  19  20  21  22\n",
            "   23  24  25  26  27  29  30  31  32  72  73  74 130 112 113 114 116 117\n",
            "  119 120 121 122 123 125 126 127 128 133 110 134 135 136 137 138 139 140\n",
            "  141 274 143 144 145 146 147 111 109  75  91  76  77  78  79  80  81  83\n",
            "   84  85  86  87  88  89  90  92 108  94  95  96  97  98  99 100 101 102\n",
            "  103 104 105 106 107 273 568 275 471 477 476 475 474 473 472 470 498 469\n",
            "  468 467 466 465 464 478 479 480 481 482 483 484 485 486 487 488 489 490\n",
            "  491 493 494 495 463 462 461 444 430 431 432 433 434 435 436 437 438 439\n",
            "  440 441 442 443 445 460 446 447 448 449 450 451 452 453 454 455 456 457\n",
            "  458 459 496 499 276 543 549 548 547 546 545 544 542 500 541 540 539 538\n",
            "  537 536 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565\n",
            "  566 535 534 533 515 501 502 503 504 505 506 507 508 509 510 511 512 513\n",
            "  514 516 532 517 518 519 520 522 523 524 525 526 527 528 529 530 531 429\n",
            "  428 427 322 328 327 326 325 324 323 321 426 319 317 316 315 313 312 329\n",
            "  330 332 333 334 336 337 339 341 342 343 344 345 346 347 348 349 311 310\n",
            "  309 291 277 278 279 280 281 282 283 567 285 286 287 288 289 290 292 308\n",
            "  293 294 295 296 297 298 299 300 302 303 304 305 306 307 350 352 353 407\n",
            "  390 391 392 393 395 396 397 398 399 400 401 403 404 405 408 387 409 410\n",
            "  411 412 413 414 416 417 418 420 421 422 423 425 388 386 354 369 355 356\n",
            "  357 358 359 360 361 362 363 364 365 366 367 368 370 385 371 372 373 374\n",
            "  375 376 377 378 379 380 381 382 383 384 284]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndzAsg2G5A49"
      },
      "source": [
        "def find_sim_comic(df, sorted_ind, c_name, top_n=10):\n",
        "    comic_name = df[df['Name'] == c_name]\n",
        "    comic_index = comic_name.index.values\n",
        "    similar_indexes = sorted_ind[comic_index, :(top_n*5)]\n",
        "    similar_indexes = similar_indexes.reshape(-1)\n",
        "    print(similar_indexes)\n",
        "    return df.iloc[similar_indexes][:top_n]\n",
        "    # 마지막 대신 이거 쓰면 return df.iloc[similar_indexes].sort_values('Rating', ascending=False)[:top_n] 평점순으로"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "KBG4K2r56djP",
        "outputId": "d0212e2f-3271-47df-b433-e599e9567423"
      },
      "source": [
        "# 'Death of a Pop Star'와 비슷한 키워드 가진 웹툰 10개 추천\n",
        "similar_comic = find_sim_comic(comic_df, Summary_sim_sorted_ind, 'Death of a Pop Star',10)\n",
        "similar_comic[['Name', 'Rating' , 'Summary_Keywords', 'Summary_Keywords_Synsets', 'Summary_Keywords_3']]"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 30 153 357 220 146 238 272 452 310 334 470 174   6  72 265 180 545  16\n",
            " 241  80  22 379 520 519  96 465 345  29  74 375  42 101 540   1 248  93\n",
            " 222 185  78 450  10 483 126  41 337 182   8 195 253 115]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Name</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Summary_Keywords</th>\n",
              "      <th>Summary_Keywords_Synsets</th>\n",
              "      <th>Summary_Keywords_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Death of a Pop Star</td>\n",
              "      <td>9.34</td>\n",
              "      <td>[reaper, lucky, build, dead]</td>\n",
              "      <td>[reaper, harvester, Grim_Reaper, Reaper, lucky...</td>\n",
              "      <td>[reaper, lucky, build]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>Metaphorical HER</td>\n",
              "      <td>8.95</td>\n",
              "      <td>[builds, visual, criminals]</td>\n",
              "      <td>[builds, physique, build, body-build, habitus,...</td>\n",
              "      <td>[builds, visual, criminals]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>UndeadEd</td>\n",
              "      <td>9.69</td>\n",
              "      <td>[dead, things]</td>\n",
              "      <td>[dead, all_in, beat, bushed, numb, deadened, i...</td>\n",
              "      <td>[dead, things]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>When Jasy Whistles</td>\n",
              "      <td>9.76</td>\n",
              "      <td>[times, asuncion, dead, jasy]</td>\n",
              "      <td>[times, multiplication, time, clip, clock_time...</td>\n",
              "      <td>[times, asuncion, dead]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>Haxor</td>\n",
              "      <td>9.11</td>\n",
              "      <td>[wrong, frame]</td>\n",
              "      <td>[wrong, wrongfulness, legal_injury, damage, in...</td>\n",
              "      <td>[wrong, frame]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>Lovely Hell</td>\n",
              "      <td>9.09</td>\n",
              "      <td>[earth, powerful, figure]</td>\n",
              "      <td>[earth, Earth, world, globe, ground, land, dry...</td>\n",
              "      <td>[earth, powerful, figure]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>The Wolf &amp; Red Riding Hood</td>\n",
              "      <td>9.63</td>\n",
              "      <td>[anna, super, figure, school]</td>\n",
              "      <td>[anna, super, superintendent, ace, A-one, crac...</td>\n",
              "      <td>[anna, super, figure]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>Kind of Love</td>\n",
              "      <td>9.64</td>\n",
              "      <td>[short]</td>\n",
              "      <td>[short, short_circuit, shortstop, short-change...</td>\n",
              "      <td>[short]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>Firebrand</td>\n",
              "      <td>8.92</td>\n",
              "      <td>[natali, world politically, perfectly, evil, m...</td>\n",
              "      <td>[natali, world politically, perfectly, absolut...</td>\n",
              "      <td>[natali, world politically, perfectly]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>334</th>\n",
              "      <td>So I Married the Anti-Fan</td>\n",
              "      <td>9.42</td>\n",
              "      <td>[hate house, suddenly, truth]</td>\n",
              "      <td>[hate house, suddenly, all_of_a_sudden, of_a_s...</td>\n",
              "      <td>[hate house, suddenly, truth]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Name  ...                      Summary_Keywords_3\n",
              "30          Death of a Pop Star  ...                  [reaper, lucky, build]\n",
              "153            Metaphorical HER  ...             [builds, visual, criminals]\n",
              "357                    UndeadEd  ...                          [dead, things]\n",
              "220          When Jasy Whistles  ...                 [times, asuncion, dead]\n",
              "146                       Haxor  ...                          [wrong, frame]\n",
              "238                 Lovely Hell  ...               [earth, powerful, figure]\n",
              "272  The Wolf & Red Riding Hood  ...                   [anna, super, figure]\n",
              "452                Kind of Love  ...                                 [short]\n",
              "310                   Firebrand  ...  [natali, world politically, perfectly]\n",
              "334   So I Married the Anti-Fan  ...           [hate house, suddenly, truth]\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    }
  ]
}